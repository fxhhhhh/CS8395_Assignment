# Generative AI Benchmarking Tool

This repository provides tools to benchmark the performance of generative AI solutions against a dataset of coding problems. The benchmarking script compares the generated solution with the expected solution and assigns a score based on their similarity.

## Setup

### Dependencies:
- Python 3.x
- OpenAI API: Required to generate solutions using OpenAI's models.

### Directory Structure:

c/problem: Contains problem prompts.
- /solution: Contains corresponding solutions to the problems.

### Scripts:
- bench-mark.py: Main Python script to run the benchmarking process.

## Usage

- Place problem prompts in the /problem directory with the naming convention problem_<number>.txt.

- Place the corresponding solutions in the /solution directory with the naming convention solution_<number>.txt.

- Run the benchmarking script:
```bash
python bench-mark.py
```
The script will generate solutions for each problem using the generative AI model, compare it to the expected solution, and then display the results.

## Evaluation Methods
The benchmarking script supports multiple methods to evaluate the similarity between the expected and generated solutions:

- Length Comparison: Compares the length of the expected and generated code.
- Jaccard Similarity: Compares the similarity based on the number of different words.
- Levenshtein Distance: Measures the minimum number of single-character edits required to change one string into the other.

By default, the benchmark uses the Levenshtein Distance. You can switch the evaluation method by uncommenting the desired method in the bench-mark.py script.

## Results
The results will display:

Problem Prompt: The given problem statement.
Expected Solution: The expected code solution from the /solution directory.
Generated Solution: The solution generated by the generative AI model.
Similarity Score: A score closer to 1 indicates high similarity between the generated and expected solution, while a score closer to 0 indicates low similarity.

## Questions

- What unique aspect of coding ability are you assessing?
    - Answer: We are assessing the capability of generative AI models to produce coding solutions that not only solve a problem but are also syntactically and semantically similar to a known correct solution. This goes beyond just "correctness"; it evaluates the AI's capacity to produce answers that mimic human-like problem-solving patterns.

- What is new and different about your problems that will give us new insights?
   - Answer: While traditional assessments like LeetCode focus on whether a solution is correct or efficient, our benchmarking approach considers how "human-like" or "canonical" the generated solution is. This provides insights into the AI's understanding of common programming idioms and best practices. For instance, for a problem that can be solved both recursively and iteratively, does the AI consistently choose one approach over the other, or does it switch based on the problem's nuances?

- How will you create data that has a known ground truth label for each problem?
   - Answer: Each problem in our dataset has a corresponding solution written by a human expert, which serves as the ground truth. The benchmarking tool then compares the AI-generated solution to this ground truth to determine its score. This ground truth is not just about a correct answer but represents a canonical or commonly accepted way to solve the problem.

- Does ground truth even matter?
   - Answer: Yes, ground truth matters, especially in this context. While generative AI might produce a solution that works, it's crucial to benchmark its output against a known standard to ensure that the AI isn't just generating code that "happens to work." By comparing against a ground truth, we can evaluate whether the AI is genuinely understanding and replicating human problem-solving patterns or merely finding shortcuts that might not generalize well across various problems.



## License
This project is licensed under the MIT License - see the LICENSE file for details.

## Authors
Cecilia Fu









